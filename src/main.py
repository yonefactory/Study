import requests
from bs4 import BeautifulSoup
import openai
import spacy
import time
from telegram_bot import send_telegram_message
from config import OPENAI_API_KEY, NEWS_URL

nlp = spacy.load("en_core_web_sm")

client = openai.OpenAI(api_key=OPENAI_API_KEY)

def request_with_retry(prompt, model="gpt-3.5-turbo", retries=5, delay=10):
    """OpenAI API ìš”ì²­ ì‹œ Rate Limit ì˜¤ë¥˜ ë°œìƒ ì‹œ ìë™ ì¬ì‹œë„"""
    for i in range(retries):
        try:
            response = client.chat.completions.create(
                model=model,
                messages=[{"role": "system", "content": prompt}]
            )
            return response.choices[0].message.content.strip()
        except openai.RateLimitError:
            print(f"âš ï¸ API ìš”ì²­ ì œí•œ. {delay}ì´ˆ í›„ ë‹¤ì‹œ ì‹œë„ ({i+1}/{retries})...")
            time.sleep(delay)  
    raise Exception("ğŸš¨ API ìš”ì²­ ì‹¤íŒ¨: Rate Limit ì´ˆê³¼")

def get_latest_news():
    """ìµœì‹  ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
    try:
        response = requests.get(NEWS_URL)
        response.raise_for_status()
    except requests.RequestException as e:
        print(f"ğŸš¨ ë‰´ìŠ¤ í˜ì´ì§€ ë¡œë”© ì‹¤íŒ¨: {e}")
        return None, None

    soup = BeautifulSoup(response.text, "html.parser")
    article = soup.find("a", class_="container__link")
    if not article:
        return None, None

    article_url = "https://edition.cnn.com" + article["href"]
    try:
        article_response = requests.get(article_url)
        article_response.raise_for_status()
    except requests.RequestException:
        return None, None

    article_soup = BeautifulSoup(article_response.text, "html.parser")
    paragraphs = article_soup.find_all("p")

    title = article_soup.find("h1").text if article_soup.find("h1") else None
    content = " ".join([p.text for p in paragraphs[:5]]) if paragraphs else None
    return title, content

def translate_text(text, target_language="ko"):
    """GPTë¥¼ ì‚¬ìš©í•´ í…ìŠ¤íŠ¸ ë²ˆì—­"""
    if not text:
        return "ë²ˆì—­í•  ë‚´ìš© ì—†ìŒ"
    prompt = f"Translate the following text to {target_language}:\n\n{text}"
    return request_with_retry(prompt, model="gpt-3.5-turbo")

def extract_keywords(sentence):
    """í•µì‹¬ ë¬¸ì¥ì—ì„œ ë‹¨ì–´ ì¶”ì¶œ"""
    if not sentence:
        return []
    doc = nlp(sentence)
    keywords = [token.text for token in doc if token.pos_ in ["NOUN", "VERB", "ADJ"] and len(token.text) > 3]
    return keywords[:3]

def generate_expressions():
    """GPTë¥¼ ì‚¬ìš©í•´ í‘œí˜„(ìˆ™ì–´, ê´€ìš©ì–´) ìƒì„±"""
    prompt = "Generate three commonly used English expressions, including idioms or phrasal verbs."
    return request_with_retry(prompt, model="gpt-3.5-turbo").split("\n")

def generate_conversation(phrase):
    """GPTë¥¼ í™œìš©í•´ í•´ë‹¹ í‘œí˜„ì„ í¬í•¨í•œ ì§§ì€ ëŒ€í™” ì˜ˆì œ ìƒì„±"""
    prompt = f"Create a short dialogue using the phrase '{phrase}'."
    return request_with_retry(prompt, model="gpt-3.5-turbo")

def fetch_valid_news_data(max_retries=3):
    """ìµœì†Œí•œ í•˜ë‚˜ ì´ìƒì˜ ìœ íš¨í•œ ë°ì´í„°(ë‹¨ì–´/í‘œí˜„)ë¥¼ í™•ë³´í•  ë•Œê¹Œì§€ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
    for attempt in range(max_retries):
        news_title, news_content = get_latest_news()
        if not news_title or not news_content:
            print(f"âš ï¸ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨. {attempt+1}/{max_retries}ë²ˆì§¸ ì¬ì‹œë„...")
            continue

        summary_sentence = request_with_retry(f"Summarize this in one sentence:\n{news_content}")
        summary_sentence_ko = translate_text(summary_sentence, target_language="ko")

        important_terms = extract_keywords(summary_sentence)
        expressions = generate_expressions()

        if important_terms and expressions:  # ìµœì†Œ í•˜ë‚˜ ì´ìƒ í™•ë³´ë˜ì—ˆëŠ”ì§€ í™•ì¸
            return news_title, summary_sentence, summary_sentence_ko, important_terms, expressions

        print(f"âš ï¸ ìœ íš¨í•œ ë‹¨ì–´ ë˜ëŠ” í‘œí˜„ì´ ë¶€ì¡±í•¨. {attempt+1}/{max_retries}ë²ˆì§¸ ì¬ì‹œë„...")

    return "No News Available", "No Summary Available", "ìš”ì•½í•  ë‰´ìŠ¤ ì—†ìŒ", [], []

# ì‹¤í–‰
news_title, summary_sentence, summary_sentence_ko, important_terms, expressions = fetch_valid_news_data()

if not important_terms or not expressions:
    send_telegram_message("âš ï¸ ì˜¤ëŠ˜ì€ ì ì ˆí•œ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‚´ì¼ ë‹¤ì‹œ í™•ì¸í•´ ì£¼ì„¸ìš”.")
    exit()

# ì•„ì¹¨ í•™ìŠµ (ë‹¨ì–´), ì˜¤í›„ í•™ìŠµ (í‘œí˜„)ìœ¼ë¡œ ë¶„ë¦¬
selected_morning_word = important_terms[0]
selected_afternoon_expression = expressions[0]

# ëŒ€í™” ì˜ˆì œ ìƒì„±
morning_conversation = generate_conversation(selected_morning_word)
afternoon_conversation = generate_conversation(selected_afternoon_expression)

# Telegram ë©”ì‹œì§€ ì „ì†¡
full_message = (
    "ğŸ“š *ì˜¤ëŠ˜ì˜ ì˜ì–´ í•™ìŠµ*\n\n"
    "ğŸ“° *ì˜¤ëŠ˜ì˜ ë‰´ìŠ¤ í—¤ë“œë¼ì¸:*\n"
    + news_title + "\nğŸ“Œ " + translate_text(news_title, target_language="ko") + "\n\n"
    "ğŸ’¡ *ì˜¤ëŠ˜ì˜ í•µì‹¬ ë¬¸ì¥:* " + summary_sentence + "\nğŸ“Œ " + summary_sentence_ko + "\n\n"
    "ğŸ” *ì˜¤ëŠ˜ì˜ ë‹¨ì–´ ë° í‘œí˜„:*\n"
    "- *ë‹¨ì–´:* " + selected_morning_word + "\n"
    "- *í‘œí˜„:* " + selected_afternoon_expression + "\n\n"
    "---\n\n"
    "ğŸŒ… *ì•„ì¹¨ í•™ìŠµ (ë‹¨ì–´):* " + selected_morning_word + "\n"
    "ğŸ’¬ *ëŒ€í™” ì†ì—ì„œ ë°°ìš°ê¸°:*\n" + morning_conversation + "\n\n"
    "---\n\n"
    "ğŸŒ‡ *ì˜¤í›„ í•™ìŠµ (í‘œí˜„):* " + selected_afternoon_expression + "\n"
    "ğŸ’¬ *ëŒ€í™” ì†ì—ì„œ ë°°ìš°ê¸°:*\n" + afternoon_conversation + "\n\n"
    "---\n\n"
    "ğŸŒ™ *ì €ë… ë³µìŠµ ì‹œê°„*\n"
    "ğŸ“– *ì˜¤ëŠ˜ ë°°ìš´ ë‹¨ì–´ ë° í‘œí˜„:*\n"
    "- " + selected_morning_word + "\n"
    "- " + selected_afternoon_expression + "\n"
    "âœ… *ì˜¤ëŠ˜ ë°°ìš´ í‘œí˜„ì„ í™œìš©í•˜ì—¬ ë¬¸ì¥ì„ ë§Œë“¤ì–´ ë³´ì„¸ìš”!*"
)

send_telegram_message(full_message)
