import requests
from bs4 import BeautifulSoup
import openai
import spacy
import time
from telegram_bot import send_telegram_message
from config import OPENAI_API_KEY, NEWS_URL

nlp = spacy.load("en_core_web_sm")

# OpenAI ìµœì‹  API ì‚¬ìš©ì„ ìœ„í•´ client ìƒì„±
client = openai.OpenAI(api_key=OPENAI_API_KEY)

def request_with_retry(prompt, model="gpt-3.5-turbo", retries=5, delay=10):
    """OpenAI API ìš”ì²­ ì‹œ Rate Limit ì˜¤ë¥˜ ë°œìƒ ì‹œ ìë™ ì¬ì‹œë„"""
    for i in range(retries):
        try:
            response = client.chat.completions.create(
                model=model,
                messages=[{"role": "system", "content": prompt}]
            )
            return response.choices[0].message.content.strip()
        except openai.RateLimitError:
            print(f"âš ï¸ API ìš”ì²­ ì œí•œ. {delay}ì´ˆ í›„ ë‹¤ì‹œ ì‹œë„ ({i+1}/{retries})...")
            time.sleep(delay)  # ì¬ì‹œë„ ì „ ëŒ€ê¸°
    raise Exception("ğŸš¨ API ìš”ì²­ ì‹¤íŒ¨: Rate Limit ì´ˆê³¼")

def get_latest_news():
    """ë¯¸êµ­ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ì—ì„œ ìµœì‹  ê¸°ì‚¬ ê°€ì ¸ì˜¤ê¸°"""
    response = requests.get(NEWS_URL)
    soup = BeautifulSoup(response.text, "html.parser")

    # ë‰´ìŠ¤ ê¸°ì‚¬ ë§í¬ ì¶”ì¶œ
    article = soup.find("a", class_="container__link")
    article_url = "https://edition.cnn.com" + article["href"]

    # ê¸°ì‚¬ ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸°
    article_response = requests.get(article_url)
    article_soup = BeautifulSoup(article_response.text, "html.parser")
    paragraphs = article_soup.find_all("p")

    # ìƒìœ„ 5ë¬¸ì¥ë§Œ ê°€ì ¸ì˜¤ê¸°
    title = article_soup.find("h1").text
    content = " ".join([p.text for p in paragraphs[:5]])
    return title, content

def extract_core_sentences(content):
    """ë‰´ìŠ¤ì—ì„œ ì¤‘ìš”í•œ ì²« 3ë¬¸ì¥ + ë§ˆì§€ë§‰ 1ë¬¸ì¥ë§Œ ì„ íƒí•˜ì—¬ ì••ì¶•"""
    sentences = content.split(". ")  # ë¬¸ì¥ì„ ë¶„ë¦¬
    if len(sentences) > 4:
        return ". ".join(sentences[:3] + [sentences[-1]])  # ì• 3ë¬¸ì¥ + ë§ˆì§€ë§‰ ë¬¸ì¥ë§Œ ì„ íƒ
    return content  # ë¬¸ì¥ì´ 4ê°œ ì´í•˜ë¼ë©´ ì›ë¬¸ ìœ ì§€

def summarize_news(content):
    """ë‰´ìŠ¤ í•µì‹¬ ë¬¸ì¥ë§Œ GPTì— ì „ë‹¬í•˜ì—¬ ìš”ì•½ (í† í° ì ˆì•½)"""
    compressed_content = extract_core_sentences(content)  # í…ìŠ¤íŠ¸ ì••ì¶•
    prompt = "Summarize the following key sentences in one concise sentence:\n\n" + compressed_content
    return request_with_retry(prompt, model="gpt-3.5-turbo")

def translate_text(text, target_language="ko"):
    """GPTë¥¼ ì‚¬ìš©í•´ í…ìŠ¤íŠ¸ ë²ˆì—­"""
    prompt = "Translate the following text to " + target_language + ":\n\n" + text
    return request_with_retry(prompt, model="gpt-3.5-turbo")

def extract_keywords(sentence):
    """í•µì‹¬ ë¬¸ì¥ì—ì„œ ì¤‘ìš”í•œ ë‹¨ì–´ ë° í‘œí˜„ ì¶”ì¶œ"""
    doc = nlp(sentence)
    keywords = [token.text for token in doc if token.pos_ in ["NOUN", "VERB", "ADJ"] and len(token.text) > 3]
    return keywords[:3]

def generate_expressions():
    """ìì£¼ ì“°ì´ëŠ” ì˜ì–´ í‘œí˜„(êµ¬, ê´€ìš©ì–´) ìƒì„±"""
    prompt = "Generate three commonly used English expressions, including idioms or phrasal verbs."
    return request_with_retry(prompt, model="gpt-3.5-turbo")

def define_terms(terms):
    """3ê°œì˜ ë‹¨ì–´ ë˜ëŠ” í‘œí˜„ì„ í•œ ë²ˆì˜ ìš”ì²­ìœ¼ë¡œ ì •ì˜ (ì˜ì–´ ì„¤ëª… + í•œêµ­ì–´ ë²ˆì—­)"""
    prompt = "Explain the following words or expressions in English and translate their meaning into Korean:\n\n"
    for term in terms:
        prompt += "- " + term + "\n"
    return request_with_retry(prompt, model="gpt-3.5-turbo")

def select_important_terms(terms):
    """GPTë¥¼ í™œìš©í•˜ì—¬ ê°€ì¥ ì¤‘ìš”í•œ ë‹¨ì–´/í‘œí˜„ 2ê°œë¥¼ ì„ íƒ"""
    prompt = "From the following words and expressions, select the two most essential and frequently used ones:\n\n"
    prompt += "\n".join(terms)
    prompt += "\n\nReturn only the two selected expressions in a comma-separated format."

    selected_terms = request_with_retry(prompt, model="gpt-3.5-turbo")
    return selected_terms.split(", ")  # ì¤‘ìš” ë‹¨ì–´ 2ê°œ ë°˜í™˜

def generate_quiz(phrase):
    """GPTë¥¼ í™œìš©í•´ ë¹ˆì¹¸ ì±„ìš°ê¸° í€´ì¦ˆ ìƒì„±"""
    prompt = f"Create a fill-in-the-blank quiz using the phrase '{phrase}'. The sentence should be natural and have a missing word for the learner to guess."
    return request_with_retry(prompt, model="gpt-3.5-turbo")

def generate_conversation(phrase):
    """GPTë¥¼ í™œìš©í•´ í•´ë‹¹ í‘œí˜„ì„ í¬í•¨í•œ ì§§ì€ ëŒ€í™” ì˜ˆì œ ìƒì„±"""
    prompt = f"Create a short dialogue using the phrase '{phrase}' in a natural conversation."
    return request_with_retry(prompt, model="gpt-3.5-turbo")

# ì‹¤í–‰
news_title, news_content = get_latest_news()
summary_sentence = summarize_news(news_content)
summary_sentence_ko = translate_text(summary_sentence, target_language="ko")
important_terms = extract_keywords(summary_sentence)  # í•µì‹¬ ë‹¨ì–´ & í‘œí˜„ ì¶”ì¶œ
expressions = generate_expressions().split("\n")  # ì¶”ê°€ì ì¸ ì˜ì–´ í‘œí˜„ ìƒì„±
all_terms = important_terms + expressions  # ì „ì²´ í•™ìŠµ ëŒ€ìƒ ë¦¬ìŠ¤íŠ¸

# ğŸ“Œ ê°€ì¥ ì¤‘ìš”í•œ ë‹¨ì–´ & í‘œí˜„ 2ê°œ ì„ ì •
selected_morning_phrase, selected_afternoon_phrase = select_important_terms(all_terms)

# í•œêµ­ì–´ ë²ˆì—­ í¬í•¨í•œ ì •ì˜ ìƒì„±
term_definitions = define_terms(all_terms)

# ì•„ì¹¨ í•™ìŠµ (ë¹ˆì¹¸ ì±„ìš°ê¸° í€´ì¦ˆ) & ì˜¤í›„ í•™ìŠµ (ëŒ€í™” ì˜ˆë¬¸) ì¶”ê°€
morning_quiz = generate_quiz(selected_morning_phrase)
afternoon_conversation = generate_conversation(selected_afternoon_phrase)

# Telegram ë©”ì‹œì§€ ì „ì†¡
send_telegram_message(f"""
ğŸ“š *ì˜¤ëŠ˜ì˜ ì˜ì–´ í•™ìŠµ*

ğŸ“° *ì˜¤ëŠ˜ì˜ ë‰´ìŠ¤ í—¤ë“œë¼ì¸:*
{news_title}
ğŸ“Œ {translate_text(news_title, target_language="ko")}

ğŸ’¡ *ì˜¤ëŠ˜ì˜ í•µì‹¬ ë¬¸ì¥:*
{summary_sentence}
ğŸ“Œ {summary_sentence_ko}

ğŸ” *ì˜¤ëŠ˜ì˜ ë‹¨ì–´ ë° í‘œí˜„:*
{term_definitions}

---

ğŸŒ… *ì•„ì¹¨ í•™ìŠµ í‘œí˜„:* {selected_morning_phrase}
ğŸ“ *ì„¤ëª…:* {term_definitions.split("\n")[all_terms.index(selected_morning_phrase)]}
â“ *ë¹ˆì¹¸ ì±„ìš°ê¸° í€´ì¦ˆ:*
{morning_quiz}
âœï¸ *ë¹ˆì¹¸ì— ì•Œë§ì€ ë‹¨ì–´ë¥¼ ì±„ì›Œë³´ì„¸ìš”!*

---

ğŸŒ‡ *ì˜¤í›„ í•™ìŠµ í‘œí˜„:* {selected_afternoon_phrase}
ğŸ“ *ì„¤ëª…:* {term_definitions.split("\n")[all_terms.index(selected_afternoon_phrase)]}
ğŸ’¬ *ëŒ€í™” ì†ì—ì„œ ë°°ìš°ê¸°:*
{afternoon_conversation}
ğŸ“ *ì´ í‘œí˜„ì„ í¬í•¨í•œ ìì‹ ë§Œì˜ ëŒ€í™”ë¥¼ ë§Œë“¤ì–´ë³´ì„¸ìš”!*

---

ğŸŒ™ *ì €ë… ë³µìŠµ ì‹œê°„*
ğŸ’¬ *ì˜¤ëŠ˜ ë°°ìš´ í•µì‹¬ ë¬¸ì¥:* {summary_sentence}
ğŸ“Œ {summary_sentence_ko}
ğŸ“– *ì˜¤ëŠ˜ ë°°ìš´ í‘œí˜„:*
- {selected_morning_phrase}
- {selected_afternoon_phrase}
âœ… *ì˜¤ëŠ˜ ë°°ìš´ í‘œí˜„ì„ í™œìš©í•˜ì—¬ ë¬¸ì¥ì„ ë§Œë“¤ì–´ ë³´ì„¸ìš”!*
""")
